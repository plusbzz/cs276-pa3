{
 "metadata": {
  "name": "pa3_play"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rank0 import *\n",
      "from doc_utils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(q,f) = extractFeatures(\"trainQD\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import os\n",
      "from math import log\n",
      "import re\n",
      "\n",
      "class Document(object):\n",
      "    '''Container class for utility static methods'''\n",
      "    @staticmethod\n",
      "    def compute_tf_vector(words,multiplier = 1):\n",
      "        tf = {}\n",
      "        for w in words:\n",
      "            if w not in tf: tf[w] = 0.0\n",
      "            tf[w] += 1\n",
      "        if multiplier > 1:\n",
      "            for w in tf: tf[w] *= multiplier\n",
      "        return tf\n",
      "    \n",
      "    @staticmethod\n",
      "    def compute_tf_norm_vector(words,l):\n",
      "        l=float(l)\n",
      "        tf = Document.compute_tf_vector(words)\n",
      "        for w in tf:\n",
      "            tf[w] /= l\n",
      "        return tf\n",
      "    \n",
      "    @staticmethod\n",
      "    def compute_tf_idf_vector(words,corpus = None):\n",
      "        tf = Document.compute_tf_vector(words)\n",
      "        if corpus is not None:\n",
      "            for w in tf:\n",
      "                tf[w] *= corpus.get_IDF(w)\n",
      "        return tf\n",
      "    \n",
      "    @staticmethod\n",
      "    def cosine_sim(tf1,tf2):\n",
      "        tot = 0.0\n",
      "        for term in [k for k in tf1 if k in tf2]:\n",
      "            tot += (tf1[term]*tf2[term])\n",
      "        return tot\n",
      "\n",
      "class CorpusInfo(object):\n",
      "    '''Represents a corpus, which can be queried for IDF of a term'''\n",
      "    def __init__(self,corpus_root_dir): # for Laplace smoothing\n",
      "        self.corpus_dir = corpus_root_dir\n",
      "        self.total_file_count = 1.0\n",
      "        self.df_counter = Counter()   # term -> doc_freq\n",
      "        \n",
      "    def compute_doc_freqs(self):\n",
      "        root = self.corpus_dir\n",
      "        for d in sorted(os.listdir(root)):\n",
      "          print >> sys.stderr, 'processing dir: ' + d\n",
      "          dir_name = os.path.join(root, d) \n",
      "          term_doc_list = []\n",
      "          \n",
      "          for f in sorted(os.listdir(dir_name)):\n",
      "            self.total_file_count += 1\n",
      "            \n",
      "            # Add 'dir/filename' to doc id dictionary\n",
      "            file_name = os.path.join(d, f)\n",
      "\n",
      "            fullpath = os.path.join(dir_name, f)\n",
      "            \n",
      "            with open(fullpath, 'r') as infile:\n",
      "                lines = [line for line in infile.readlines()]\n",
      "                tokens = set(reduce(lambda x,line: x+line.strip().split(),lines,[]))   \n",
      "                for token in tokens: self.df_counter[token] += 1\n",
      "    \n",
      "    def get_IDF(self,term):\n",
      "        return log(self.df_counter[term]+1.0) - log(self.total_file_count) # for Laplace smoothing\n",
      "\n",
      "class Anchor(object):\n",
      "    '''Properties of a single anchor text chunk'''\n",
      "    def __init__(self,anchor_text,anchor_count):\n",
      "        self.text = anchor_text\n",
      "        self.terms = self.text.strip().split()\n",
      "        self.count = anchor_count\n",
      "        self.term_counts = Document.compute_tf_vector(self.terms,self.count)\n",
      "            \n",
      "     \n",
      "class Page(object):\n",
      "    \n",
      "    fields = ['url','header','body','anchor','title']\n",
      "    \n",
      "    '''Represents a single web page, with all its fields. Contains TF vectors for the fields'''\n",
      "    def __init__(self,page,page_fields):\n",
      "        self.url = page\n",
      "        self.body_length = page_fields.get('body_length',0)\n",
      "        self.pagerank = page_fields.get('pagerank',0)\n",
      "        self.title = page_fields.get('title',\"\")\n",
      "        self.header = page_fields.get('header',\"\")\n",
      "        self.body_hits = page_fields.get('body_hits',0)\n",
      "        self.anchors = [Anchor(text,count) for text,count in page_fields.get('anchors',{}).iteritems()]\n",
      "        self.field_tf_vectors = self.compute_field_tf_vectors()\n",
      "\n",
      "    def url_tf_vector(self): # TODO parse/split URL\n",
      "        words = filter(lambda x: len(x) > 0,re.split('\\W',self.url))\n",
      "        return Document.compute_tf_norm_vector(words,self.body_length)\n",
      "\n",
      "    def header_tf_vector(self):\n",
      "        words = reduce(lambda x,h: x+h.strip().split(),self.header,[])\n",
      "        return Document.compute_tf_norm_vector(words,self.body_length)\n",
      "\n",
      "    def body_tf_vector(self):\n",
      "        tf = {}\n",
      "        l = float(self.body_length)       \n",
      "        for bh in self.body_hits:\n",
      "            tf[bh] = len(self.body_hits[bh])/l       \n",
      "        return tf\n",
      "\n",
      "    def title_tf_vector(self): \n",
      "        words = self.title.strip().split() # Can do stemming etc here\n",
      "        return Document.compute_tf_norm_vector(words,self.body_length)\n",
      "\n",
      "    def anchor_tf_vector(self):\n",
      "        tf = {}\n",
      "        for a in self.anchors:\n",
      "            atf = a.term_counts\n",
      "            for term in atf:\n",
      "                if term not in tf: tf[term] = 0.0\n",
      "                tf[term] += atf[term]\n",
      "        for term in tf: tf[term] /= self.body_length # normalize\n",
      "        return tf\n",
      "\n",
      "    def compute_field_tf_vectors(self):\n",
      "        tfs = {}\n",
      "        tfs['url']      = self.url_tf_vector()    # TODO\n",
      "        tfs['header']   = self.header_tf_vector()\n",
      "        tfs['body']     = self.body_tf_vector()   \n",
      "        tfs['title']    = self.title_tf_vector()\n",
      "        tfs['anchor']   = self.anchor_tf_vector() # TODO\n",
      "        return tfs\n",
      " \n",
      "class QueryPage(object):  \n",
      "    field_weights = {\n",
      "        'url'   :   1.0,\n",
      "        'header':   1.0,\n",
      "        'body'  :   1.0,\n",
      "        'anchor':   1.0,\n",
      "        'title' :   1.0    \n",
      "    }\n",
      "    \n",
      "    def __init__(self,query,page):\n",
      "        self.query = query\n",
      "        self.page = page\n",
      "        self.field_scores = {}\n",
      "        self.final_score = 0.0\n",
      "        self.compute_cosine_scores()\n",
      "        \n",
      "    def compute_cosine_scores(self):\n",
      "        tf1 = self.query.tf_vector\n",
      "        for field in QueryPage.field_weights:\n",
      "            tf2 = self.page.field_tf_vectors[field]\n",
      "            self.field_scores[field] = Document.cosine_sim(tf1,tf2)\n",
      "            self.final_score += (QueryPage.field_weights[field] * self.field_scores[field])\n",
      "        \n",
      "# Look in rank0.main() for how this object is created. Also look at the pa3_play ipython notebook.\n",
      "class Query(object):\n",
      "\n",
      "    '''A single query, with all the results associated with it'''\n",
      "    def __init__(self,query,query_pages,corpus=None):  # query_pages : query -> urls\n",
      "        self.query = query\n",
      "        self.terms = self.query.strip().split()\n",
      "        self.pages = dict([(p,Page(p,v)) for p,v in query_pages.iteritems()]) # URLs\n",
      "        self.tf_vector = Document.compute_tf_idf_vector(self.terms,corpus)\n",
      "        self.compute_cosine_scores()\n",
      "        \n",
      "    def compute_cosine_scores(self):\n",
      "        self.page_scores = [QueryPage(self,page) for p,page in self.pages.iteritems()]       \n",
      "        self.ranked_page_scores = [(qp.page.url,qp.final_score) for qp in sorted(self.page_scores,key=lambda x: x.final_score, reverse=True)]\n",
      "        self.ranked_pages = [qp.page.url for qp in sorted(self.page_scores,key=lambda x: x.final_score, reverse=True)]\n",
      "                \n",
      "\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = Query('manning',f['manning'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query.pages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "{'http://cyberlaw.stanford.edu/page/manning-faculty-lounge': <doc_utils.Page at 0x10227ef50>,\n",
        " 'http://engineering.stanford.edu/node/9067': <doc_utils.Page at 0x10228c0d0>,\n",
        " 'http://explorecourses.stanford.edu/instructor/manning': <doc_utils.Page at 0x10228c1d0>,\n",
        " 'http://nlp.stanford.edu/jrfinkel/': <doc_utils.Page at 0x10228c190>,\n",
        " 'http://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf': <doc_utils.Page at 0x10228c310>,\n",
        " 'http://nlp.stanford.edu/~manning/': <doc_utils.Page at 0x10227eb90>,\n",
        " 'http://nlp.stanford.edu/~manning/papers/': <doc_utils.Page at 0x10228c050>,\n",
        " 'http://online.stanford.edu/instructors/christopher-manning': <doc_utils.Page at 0x10228c290>,\n",
        " 'http://www.stanford.edu/class/cs224n/': <doc_utils.Page at 0x10228c250>,\n",
        " 'https://stanfordwho.stanford.edu/lookup?search=Chris%20Manning&submit=Search': <doc_utils.Page at 0x10227ebd0>}"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qp =query.pages['http://nlp.stanford.edu/~manning/']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a in qp.anchors: print a.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "prof chris manning\n",
        "http nlp stanford edu manning\n",
        "christopher manning profile\n",
        "manning c\n",
        "cd manning\n",
        "christopher david manning\n",
        "http www nlp stanford edu manning\n",
        "christopher d manning\n",
        "manning\n",
        "professor christopher manning\n",
        "christopher manning\n",
        "chris manning\n",
        "prof christopher manning\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qp.field_tf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "{'anchor': {'c': 0.0009940357852882703,\n",
        "  'cd': 0.0009940357852882703,\n",
        "  'chris': 0.11332007952286283,\n",
        "  'christopher': 0.09642147117296222,\n",
        "  'd': 0.02286282306163022,\n",
        "  'david': 0.0009940357852882703,\n",
        "  'edu': 0.014910536779324055,\n",
        "  'http': 0.014910536779324055,\n",
        "  'manning': 0.2276341948310139,\n",
        "  'nlp': 0.014910536779324055,\n",
        "  'prof': 0.002982107355864811,\n",
        "  'professor': 0.0009940357852882703,\n",
        "  'profile': 0.0019880715705765406,\n",
        "  'stanford': 0.014910536779324055,\n",
        "  'www': 0.0009940357852882703},\n",
        " 'body': {'manning': 0.005964214711729622},\n",
        " 'header': {'christopher': 0.0009940357852882703,\n",
        "  'manning': 0.0009940357852882703},\n",
        " 'title': {'christopher': 0.0009940357852882703,\n",
        "  'manning': 0.0009940357852882703,\n",
        "  'nlp': 0.0009940357852882703,\n",
        "  'stanford': 0.0009940357852882703},\n",
        " 'url': None}"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s=\"http://explorecourses.stanford.edu/instructor/manning\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = 'http://www.example.com:8080/abcd/dir/file1.html?query1=value1&query2=value2'\n",
      "\n",
      "\n",
      "filter(lambda x: len(x) > 0,re.split('\\W',s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "['http', 'explorecourses', 'stanford', 'edu', 'instructor', 'manning']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urlparse import urlsplit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urlsplit(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "SplitResult(scheme='http', netloc='explorecourses.stanford.edu', path='/instructor/manning', query='', fragment='')"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}